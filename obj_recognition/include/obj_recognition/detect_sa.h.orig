#ifndef DETECT_OBJECTS_H_
#define DETECT_OBJECTS_H_

//////// std includes ////////
#include <stdio.h>
#include <omp.h>

/////// ROS ///////
#include <ros/ros.h>
#include <geometry_msgs/Pose.h>
#include <std_msgs/String.h>
#include <laser_assembler/AssembleScans2.h>
#include <tf/transform_datatypes.h>
#include <tf/transform_listener.h>
#include <tf/transform_broadcaster.h>
#include <ros/publisher.h>
#include <sensor_msgs/point_cloud_conversion.h>
#include <image_transport/image_transport.h>
#include <cv_bridge/cv_bridge.h>
#include <sensor_msgs/image_encodings.h>
#include <sensor_msgs/Image.h>
#include <image_geometry/pinhole_camera_model.h>
#include <ros/package.h>
#include <image_transport/subscriber_filter.h>
#include <message_filters/subscriber.h>
#include <message_filters/sync_policies/exact_time.h>

#include <Eigen/Dense>
<<<<<<< HEAD
#include <Eigen/LU>
#include <Eigen/Eigenvalues> 
=======
>>>>>>> 8b4700e... adding drill views and detection using SA cameras


#include <opencv2/imgproc/imgproc.hpp>
#include <opencv2/highgui/highgui.hpp>
#include <math.h>


#include <pcl17_ros/point_cloud.h>
#include <pcl17_ros/transforms.h>
#include <pcl17/search/search.h>
#include <pcl17/search/kdtree.h>
#include <pcl17/segmentation/region_growing_rgb.h>
<<<<<<< HEAD
#include <pcl17/registration/transformation_estimation_svd.h>
=======
>>>>>>> 8b4700e... adding drill views and detection using SA cameras
#include <pcl17/point_types.h>
#include <pcl17/visualization/cloud_viewer.h>
/////// PCL ///////
#include <pcl17/console/parse.h>
#include <pcl17/io/pcd_io.h>
#include <pcl17/common/transforms.h>
#include <pcl17/common/pca.h>
#include <pcl17/visualization/pcl_visualizer.h>
#include <pcl17/visualization/cloud_viewer.h>
#include <pcl17/ros/conversions.h>



<<<<<<< HEAD

=======
>>>>>>> 8b4700e... adding drill views and detection using SA cameras
#include <pcl17/features/normal_3d_omp.h>
// #include <pcl17/features/our_cvfh.h>
#include <pcl17/keypoints/uniform_sampling.h>

#include <pcl17/filters/filter.h>
#include <pcl17/filters/passthrough.h>
#include <pcl17/filters/voxel_grid.h>
#include <pcl17/filters/extract_indices.h>

#include <pcl17/sample_consensus/sac.h>
#include <pcl17/sample_consensus/model_types.h>
#include <pcl17/sample_consensus/method_types.h>

#include <pcl17/segmentation/sac_segmentation.h>
#include <pcl17/segmentation/extract_clusters.h>

// #include <pcl17/recognition/cg/correspondence_grouping.h>
// #include <pcl17/recognition/cg/geometric_consistency.h>
// #include <pcl17/recognition/cg/hough_3d.h>
// #include <pcl17/recognition/cg/geometric_consistency.h>

// #include <pcl17/recognition/hv/hv_papazov.h>
// #include <pcl17/recognition/hv/hv_go.h>
// #include <pcl17/recognition/hv/greedy_verification.h>

#include <pcl17/registration/transformation_estimation_svd.h>
#include <pcl17/registration/ia_ransac.h>
#include <pcl17/registration/registration.h>
//#include <pcl17/registration/icp.h>
#include <trooper_obj_recognition/icpscale.h>
// #include <pcl17/registration/gicp.h> //generalized icp extension
// #include <pcl17/registration/icp_nl.h> //Levenberg-Marquardt icp 

/////// TROOPER includes ///////
#include <trooper_msgs/DetectedObjectList.h>
#include <trooper_adaptive_perception_msgs/ObjectDetectionRequest.h>
<<<<<<< HEAD
#include <trooper_msgs/DetectedObject.h>
#include <trooper_msgs/WorldModelOperation.h>
=======

>>>>>>> 8b4700e... adding drill views and detection using SA cameras


typedef pcl17::PointXYZ PointT; //NOTE: this is bad! (typedef in header file and not even a namespace)
typedef pcl17::PointXYZRGB PointTc; //NOTE: this is bad! (typedef in header file and not even a namespace)

struct Model
{
    int id; ///<model view id ... not really used anywhere but useful for debugging maybe
    pcl17::PointCloud<PointT>::Ptr cloud; ///<model cloud from sensor point of view
    pcl17::PointCloud<PointT>::Ptr cloud_sampled; ///<downsampled cloud (used for first phase of pose estimate)
    geometry_msgs::PosePtr pose;///<pose relative to robot during recording
};


  //points of mouse callback
  cv::Point mouse_point1_;
  cv::Point mouse_point2_;
  bool click1_;
  bool click2_;

  /// Mouse callback
  void callbackmouse(int event, int x, int y, int flags, void* param);

class ObjDetector
{
public:
    ///Default constructor
    ObjDetector();
    ///Default destructor
    ~ObjDetector(){/*empty*/}
    
    ///\brief initialize object detector by reading parameters and loading models
    ///\return false if loading of models failed or parameters wrong/missing, true otherwise
    bool initialize();
    
    ///\brief load models from database
    ///\param db_file[in] database file storing point clouds and associated poses
    ///\return true if loading was successful
    bool loadModels(const std::string &db_file);
    
    ///\brief try to match cluster
    ///\param cluster point cloud of cluster
    ///\return view id if found, -1 otherwise
    void detect(const pcl17::PointCloud< PointT >::ConstPtr& cluster, int *matchedModelID, Eigen::Matrix4f *matchedTransform, Eigen::Matrix4f *matchedTransform2, const pcl17::visualization::PCLVisualizer::Ptr &visu);
    
    ///\brief Returns model with id "id"
    void getModel(int id, Model *model);
    
    ///\brief Set max icp iterations
    void setMaxIter(int maxIter) {max_iter_ = maxIter;}
    
    ///\brief Set threshold for valid matches
    void setScoreThreshold(double thresh){score_thresh_ = thresh;}
    
  
     
private:
    ros::ServiceClient client_;
    ros::Publisher pub_;
    ros::Subscriber trigger_sub_;
    /// stereo to world transform 
    tf::StampedTransform stereo2world_;
    /// cloud to robot transform
    tf::StampedTransform cloud_to_robot_;
    /// cloud to camera
<<<<<<< HEAD
<<<<<<< HEAD
    tf::StampedTransform robot_to_camera_;
    /// robot to world
    tf::StampedTransform robot_to_world_;
=======
    tf::StampedTransform cloud_to_camera_;
>>>>>>> 8b4700e... adding drill views and detection using SA cameras
=======
    tf::StampedTransform robot_to_camera_;
  
    
>>>>>>> 2dc2ee9... fixed obj det bug caused by plane detection, renamed drill pcd model dir to match trooper name
    
    
    /// name of colored point cloud
    std::string topicLidar_;
    /// stores the object type the detector is detecting
    std::string obj_type_;
    /// robot refence system
    std::string robot_frame_id_;
    /// world reference system
    std::string world_frame_id_;
    /// stereo reference system
    std::string stereo_ref_system_;
    /// lidar reference system
    std::string cloud_frame_id_;
    /// camera reference system
    std::string camera_frame_id_;
    
    /// refence transform listener
    tf::TransformListener tf_;
    /// stereo transform
    //tf::TransformListener tf_stereo_;
    /// models for all the views of the object
    std::vector<Model> models_;
    /// cluster bounds for pre-filtering (min height, min width, max height, max width) 
    std::vector<double> cluster_bounds_; //pre-filter constraints for clusters to discard irrelevant clusters and speed up detection 
                                         //... get rid of this maybe when switching to our cvfh? Could still be useful for speed up purposes though.
                                         //We're not really expecting a lot of clutter in the two VRC scenarios though so maybe useless
    
    /// Image to send to the user
    std::vector<cv::Mat> HSV_;
    /// stereo PC params
    bool gotPCstereo_;
    bool image_ready_;
    
    boost::mutex stereo_mtx_; 
    boost::mutex image_mtx_; 
    
    pcl17::PointCloud<PointT> stereoPC_;
    image_geometry::PinholeCameraModel leftcamproj_; 
    
    sensor_msgs::ImageConstPtr l_image_msg_;    
    cv_bridge::CvImageConstPtr l_cv_ptr_;
    
    std::vector<cv::Point2f> roi_;
    cv::Size im_size_;
    
    /// Algorithm params
    double radius_search_;
    
    /// color segmentation
    bool color_segmentation_;
    double DistanceThreshold_;
    double PointColorThreshold_;
    double RegionColorThreshold_;
    double MinClusterSize_;
    /// plane segmentation threshold
    double PlanesegThres_;    
    int PlaneSize_;
    /// minimum height to detect objects
<<<<<<< HEAD
    double highplane_;
=======
    double Highplane_;
>>>>>>> 8b4700e... adding drill views and detection using SA cameras
    
    /// hose segmentation mainly
    bool manual_segmentation_;
    
   
    //window to show to the user
    const char* src_window_; 
   
    
    /// min number of points for a cluster to be considered
    int min_cluster_size_; 
    /// leaf size for voxel grid sampling of model and clusters
    double sample_size_;
    /// max distance tolerance during clustering ... needs to be larger for polaris due to large gaps when viewed from behind
    double cluster_tolerance_; 
    double score_thresh_; ///< threshold for selecting a valid match 
                          //TODO: add 2 thresholds for rough and fine match?
    int max_iter_; ///< max ICP iterations for model refinement
    bool on_table_; ///< flag whether object can be on a table ... mainly for optimization right now
    
    
    /// Transformation to the robot reference system
    //Eigen::Matrix4f cloud_to_robotE_;
    
    ///callback for receiving detection commands
    void detectCallback(const trooper_adaptive_perception_msgs::ObjectDetectionRequestConstPtr &cmd);
    
    /// callback for stereo point cloud
    void stereoPCcallback(const sensor_msgs::PointCloud2ConstPtr &stereocloud);
    /// Image callback
    void processImage(const sensor_msgs::ImageConstPtr &l_image_msg, const sensor_msgs::CameraInfoConstPtr& l_info_msg);
    
    
    ///extract clusters from scene assuming ground plane
    void extractClusters(const pcl17::PointCloud<PointT>::ConstPtr &scene, std::vector<pcl17::PointCloud<PointT>::Ptr> *clusters, const pcl17::visualization::PCLVisualizer::Ptr &visu);
    ///extract clusters based on color   
    void extractClustersColor(const pcl17::PointCloud<PointT>::ConstPtr &scene, std::vector<pcl17::PointCloud<PointT>::Ptr> *clusters);
    
    
    
    /// synchronization 
    /// image subscriber
    image_transport::SubscriberFilter image_sub_;
    /// camera info subscriber   
    message_filters::Subscriber<sensor_msgs::CameraInfo> im_info_sub_;
    /// stereo point cloud subscriber
    message_filters::Subscriber<sensor_msgs::PointCloud2> stereo_pcl_sub_;   
    
    typedef message_filters::sync_policies::ExactTime<sensor_msgs::Image, sensor_msgs::CameraInfo, sensor_msgs::PointCloud2> ExactPolicy;
    typedef message_filters::Synchronizer<ExactPolicy> ExactSync;
    boost::shared_ptr<ExactSync> exact_sync_;    

    
    /// for sync checking
    static void increment(int* value) {  ++(*value);  }
    /// checks for synchronized data streams, gets called every 15 seconds
    void checkInputsSynchronized();
    
    
    /// Countger synchronization variables
    int image_received_, im_info_received_, stereo_pcl_received_, all_received_;    
    // for sync checking
    ros::Timer check_synced_timer_;
    
     /// synchronization wrapper for data callback
    void dataCbSync(const sensor_msgs::ImageConstPtr &img_msg, 
                          const sensor_msgs::CameraInfoConstPtr &cam_info,                           
                          const sensor_msgs::PointCloud2ConstPtr &pcl_msg);
    
    
    
    /// callback function for receiving image and stereo point cloud
    void dataCallback( const sensor_msgs::ImageConstPtr &img_msg, 
         const sensor_msgs::CameraInfoConstPtr &cam_info,                       
         const sensor_msgs::PointCloud2ConstPtr &pcl_msg);
    
<<<<<<< HEAD
     /// get the rigid transformation between two point clouds using svd decomposition
    void getTransformationFromCorrelation ( const Eigen::MatrixXd &source_pc, const Eigen::Vector4d & centroid_src,
        const Eigen::MatrixXd &target_pc, const Eigen::Vector4d & centroid_tgt, Eigen::Matrix4d &transformation_matrix);
    
=======
>>>>>>> 8b4700e... adding drill views and detection using SA cameras
};

#endif
